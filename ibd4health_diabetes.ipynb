{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will do some basic data analyses. As a basis, we will use a synthetic data set containing biographical and medicinal information of 10.000 persons.\n",
    "\n",
    "We will guide you through a number of common steps in a typical data analysis project. We first briefly describe what we will do (and why) and then show the code. To execute a code block, press `ctrl + enter` and wait for the execution to finish. Typically, after the execution of a code block you will see a text print and/or relevant figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Python and data primer\n",
    "\n",
    "## 0.1. Python primer\n",
    "\n",
    "In this section we will provide a concise overview of the Python language, to enable you to get started. The '#' sign is used to start a comment in Python, that lasts until the end of the line. The 'print()' command can be used to print output. The 'type()' command shows the type of its argument. Python indexing always starts at 0. For more information about Python, please refer to https://docs.python.org/3/reference/index.html.\n",
    "\n",
    "### Primitive types\n",
    "First, lets go over some primitive types in Python: integers (whole numbers), floats (real numbers), booleans (true/false) and strings (text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# integers\n",
    "a = 1\n",
    "print(type(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# floats\n",
    "b = 1.0\n",
    "type(b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# booleans\n",
    "c = True\n",
    "print(type(c))\n",
    "print(c)\n",
    "d = a == 2\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# string\n",
    "s = 'We are the knights who say \"Ni!\"'\n",
    "print(type(s))\n",
    "print(s)\n",
    "s2 = s + ' And we demand a shrubbery!'\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in basic data types\n",
    "\n",
    "Python has a number of built-in data types, including lists, dictionaries and sets. We will briefly go over the uses and syntax of each of these.\n",
    "\n",
    "A *list* is an ordered sequence of values, not necessarily of the same type. Lists can be initialized via square brackets []."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = [1, 'blah', 2.0, True]\n",
    "print(type(l))\n",
    "print(l)\n",
    "\n",
    "# iterate over list\n",
    "for i in l:\n",
    "    print(i)\n",
    "    \n",
    "# add element to a list\n",
    "l.append(5)\n",
    "print(l)\n",
    "\n",
    "# list slicing: print the first 3 elements\n",
    "print(l[0:3])\n",
    "\n",
    "# length of the list: use the len() function\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *dictionary* (dict) is a container that maps (unique) keys to values, and is typically initialized using curly brackets {}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'a': 1, 'b': 2}\n",
    "print(type(d))\n",
    "print(d)\n",
    "\n",
    "# get item 'a' from the dictionary\n",
    "print(d['a'])\n",
    "\n",
    "# length of the dictionary\n",
    "print(len(d))\n",
    "\n",
    "# iterate over the dictionary\n",
    "# note that the ordering is not necessarily as initialized\n",
    "for k, v in d.items():\n",
    "    print('key %s mapped to value %d' % (k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *set* is a container for distinct values, and can be initialized via the *set()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = set([1, 2, 3, 3])\n",
    "print(type(s1))\n",
    "print(s1)\n",
    "\n",
    "s2 = set([3, 4, 5])\n",
    "print(s2)\n",
    "\n",
    "# set union\n",
    "s3 = s1.union(s2)\n",
    "print(s3)\n",
    "\n",
    "# set difference\n",
    "s4 = s1.difference(s2)\n",
    "print(s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various useful functions\n",
    "\n",
    "The 'help()' function can be used to open the documentation of a certain function or data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.0 represents a float object\n",
    "help(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map() is a built-in function\n",
    "help(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions provide an elegant way to create or transform iterables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to see what range() does, use help(range)\n",
    "a = [5 * i for i in range(5)]\n",
    "print(a)\n",
    "b = [x - 1 for x in a]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is importing libraries and/or data. Python has a wealth of libraries that provide well-documented functions we can use for data analysis. In particular, we will use 'NumPy' (numerical support, esp. linear algebra), 'scikit-learn' (machine learning), 'pandas' (statistics) and matplotlib (plotting). A library can be loaded using the 'import' command and renamed via 'import x as y'. To call function x from library y, we use y.x()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # numpy\n",
    "import sklearn # scikit-learn\n",
    "import pandas as pd # pandas\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a synthetic data set based on health expenditure data. The data is organized as a matrix, and contains biographical information, class labels and information about drug purchases.\n",
    "\n",
    "For your convenience, we created a small library for loading the data and some convenience functions. We will load the library as 'ibd4h'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ibd4health as ibd4h\n",
    "data = ibd4h.data\n",
    "features = ibd4h.features\n",
    "labels = ibd4h.labels\n",
    "colidx = ibd4h.colidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variables are important:\n",
    "- **data**: the data matrix, rows correspond to patients, columns to features\n",
    "- **features**: the list of features (corresponding to columns in ibd4h.data)\n",
    "- **labels**: the labels for patients (corresponding to rows, True: diabetic, False: non-diabetic)\n",
    "- **colidx**: a dictionary to facilitate retrieving columns from their string definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Data set contains info on %d patients with %d features.\" \n",
    "      % data.shape)\n",
    "print(\"Feature list has %d entries.\" \n",
    "      % features.shape)\n",
    "print(\"Label list has %d entries, of which %d are positive.\" \n",
    "      % (labels.shape[0], sum(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this data set include age, gender the volume of drugs purchased by the patient. Lets have a look at the first 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, feat in enumerate(features[:3]):\n",
    "    print('feature %d: %s' % (idx, feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drug volumes are categorized via codes of the anatomical therapeutic chemical (ATC) classification system (cfr. http://www.whocc.no/atc_ddd_index/ for details). You can find a description of ATC code XXX in the data set at http://www.whocc.no/atc_ddd_index/?showdescription=yes&code=XXX. For example, information on the third feature 'A01AA01', can be found at http://www.whocc.no/atc_ddd_index/?showdescription=yes&code=A01AA01.\n",
    "\n",
    "The data matrix can be indexed via `data[rowidx, colidx]`. For example, the age of the first patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ageidx = colidx['age']\n",
    "age = data[0, ageidx]\n",
    "print(age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For slicing, replace either rowidx or colidx with \":\". For example, to get a vector of all ages, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ages = data[:, ageidx]\n",
    "print(ages.shape)\n",
    "print('Average age: %1.3f' % np.mean(ages))\n",
    "print('Minimum age: %1.3f' % np.min(ages))\n",
    "print('Maximum age: %1.3f' % np.max(ages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory analysis\n",
    "\n",
    "Exploratory analysis is an important step in any project. This step serves several purposes, including:\n",
    "- understand the basic properties and underlying structure of the data set\n",
    "- perform sanity checks wherever possible to ensure that the data is pristine\n",
    "\n",
    "We will first check some basic properties: the gender distribution of our data set, and within the positive and negative subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genderidx = colidx['gender']\n",
    "# create a list of gender indicators via list comprehension\n",
    "genders = [x > 0 for x in data[:, genderidx]]\n",
    "\n",
    "# define gender representation\n",
    "male = False\n",
    "female = True\n",
    "\n",
    "num_males = genders.count(male)\n",
    "num_females = genders.count(female)\n",
    "print('Data set contains %d males and %d females.' % (num_males, num_females))\n",
    "\n",
    "# gender distribution in diabetics\n",
    "positive_rowidx = [idx for idx, label in enumerate(labels) if label]\n",
    "positive_genders = [x > 0 for x in data[positive_rowidx, genderidx]]\n",
    "\n",
    "num_positive_males = positive_genders.count(male)\n",
    "num_positive_females = positive_genders.count(female)\n",
    "print('Data set contains %d positive males and %d positive females.' % (num_positive_males, num_positive_females))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first test shows that the data set is fully gender balanced and class balanced. Next, lets look at the age distribution of patients in the data set. We will do this by plotting a histogram using matplotlib's [`hist()` function](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(data[:, ageidx], bins=50)\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Age histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets see if the age distributions are comparable for males and females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a mask to identify males and females in the data\n",
    "male = data[:, colidx['gender']] > 0\n",
    "female = np.invert(male)\n",
    "print(female[:5])\n",
    "print(male[:5])\n",
    "\n",
    "# create vectors of ages\n",
    "male_ages = data[male, ageidx]\n",
    "female_ages = data[female, ageidx]\n",
    "\n",
    "# plot histograms of age vectors\n",
    "plt.hist((male_ages, female_ages), bins=20)\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend(['male (mean: %1.3f)' % np.mean(male_ages),\n",
    "           'female (mean: %1.3f)' % np.mean(female_ages)],\n",
    "          bbox_to_anchor=(1.2,1.0))\n",
    "plt.title('Age histogram per gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "After stratifying the age histogram by gender, we can see that males tend to be a bit older in our data set, but the differences are not extremely large.\n",
    "\n",
    "A next step to get some intuition in the data is to assess basic pair-wise relationships between features. The function below plots the distributions of two data columns of your choice and shows a simple [least-squares](https://en.wikipedia.org/wiki/Least_squares) regression line. Finally, we also compute the [Pearon correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) between the feature pair to assess if they are related. The Pearson correlation ranges from -1 (anticorrelated) to 1 (perfectly correlated), with 0 implying there is no linear correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to plot two features to explore relationships between them\n",
    "def plot_feature_pair(feature_a, feature_b):\n",
    "    column_a = data[:, colidx[feature_a]]\n",
    "    column_b = data[:, colidx[feature_b]]\n",
    "    n = data.shape[0]\n",
    "    plt.plot(column_a, column_b, '+', mew=2)\n",
    "    plt.xlabel(feature_a)\n",
    "    plt.ylabel(feature_b)\n",
    "       \n",
    "    # to remove outliers, we will show only up to the 95% percentile\n",
    "    max_a = list(sorted(column_a))[int(0.95 * n)]\n",
    "    max_b = list(sorted(column_b))[int(0.95 * n)]\n",
    "   \n",
    "    # compute regression line\n",
    "    A = np.vstack([column_a[:], np.ones(len(column_a))]).T\n",
    "    m, c = np.linalg.lstsq(A, column_b[:])[0]\n",
    "    plt.plot(column_a, m*column_a + c, 'k', linewidth=3, label='LS regression')\n",
    "    plt.xlim(0, max_a)\n",
    "    plt.ylim(0, max_b)\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.2,1.0))\n",
    "    \n",
    "    # compute correlation between features\n",
    "    corr = np.corrcoef(np.array((column_a, column_b)))\n",
    "    print('Pearson correlation: %1.3f' % corr[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets look at the relationship between a person's age and his or her statin usage. Statins are a class of lipid-lowering medication, so we expect it to be positively correlated with age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# age and statins (simvastatin) --> strong correlation\n",
    "plot_feature_pair('age', 'C10AA01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we see a clear correlation between age and statin usage.\n",
    "\n",
    "Next, we will check two unrelated types of medication: paracetamol and statins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paracetamol and statins (simvastatin) --> weak correlation\n",
    "plot_feature_pair('N02BE01', 'C10AA01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson correlation coefficient between paracetamol and statin usage is almost 0, which indicates that no (linear) relationship exists between these two features. This confirms our expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple predictive modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we want to construct a classifier to predict whether a person will need glucose-lowering agents in the future (the labels), based on his or her recent medical history (the feature vectors). First, we must split the data into two partitions: one for training the classifier, and one for testing it.\n",
    "\n",
    "We will do this by generating a random permutation of the data, and then use the first half to train a model and the second half to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a permutation of the original data and labels\n",
    "num_patients = len(data)\n",
    "permutation_index = np.random.permutation(num_patients)\n",
    "permuted_data = data[permutation_index, :]\n",
    "permuted_labels = labels[permutation_index]\n",
    "\n",
    "# create training data and labels\n",
    "subset_size = int(num_patients / 2)\n",
    "train_data = permuted_data[:subset_size, :]\n",
    "train_labels = permuted_labels[:subset_size]\n",
    "\n",
    "# create test data and labels\n",
    "test_data = permuted_data[subset_size:, :]\n",
    "test_labels = permuted_labels[subset_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to build a predictive model. For simplicity, we will use a linear support vector machine (SVM) as described in this morning's lecture. The optimization problem of a linear SVM can be written as follows:\n",
    "\n",
    "$\\begin{align}\\min_{\\mathbf{w},\\xi,b} &= \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^N \\xi_i, \\\\\n",
    "\\text{subject to: } &y_i (\\mathbf{w}^T\\mathbf{x}_i - b) \\geq 1, \\quad \\forall i=1,\\ldots,N, \\end{align}$\n",
    "\n",
    "Where $y$ is the vector of class labels, $\\mathbf{x}_i$ is the i'th data vector, $\\mathbf{w}$ is the separating hyperplane and $\\xi$ is a vector of slack variables to allow misclassifications in the training set ($\\xi_i > 0$ if and only if instance i is misclassified and 0 otherwise). The hyperparameter $C$ controls the tradeoff between the simplicity of the model (minimizing $\\|\\mathbf{w}\\|^2$) and classification accuracy on the training set (minimizing $\\sum_{i=1}^N \\xi_i$).\n",
    "\n",
    "A linear SVM computes a hyperplane that optimally separates the two classes by maximizing the margin, that is the between the separating hyperplane $\\mathbf{w}$ and instances of either class, while some instances may be inside the margin (uncertain classifications) or on the wrong side of the separating hyperplane (misclassifications). Increasing the hyperparameter puts more emphasis on minimizing the amount of misclassified instances and instances within the margin.\n",
    "\n",
    "We will use the implementation of linear SVM classifiers that is available in [sklearn.svm.LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the LinearSVC function\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# create a linear SVM with default parameters (C=1.0)\n",
    "svm = LinearSVC()\n",
    "\n",
    "# train the model using the training data\n",
    "svm.fit(X=train_data, y=train_labels) \n",
    "\n",
    "# predict the test data using our model\n",
    "# we use decision_function() to compute the (signed) distance to the separating hyperplane for each test instance\n",
    "test_predictions = svm.decision_function(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets have a qualitative look at our SVM's predictions, by plotting a histogram of the predictions of the positive and negative class. Hopefully, the predicted decision values for the positive test instances or structurally larger than the those for the negative test instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get indices of the positive and negative instances in the test set\n",
    "positive_test_indices = [i for i, x in enumerate(test_labels) if x]\n",
    "negative_test_indices = [i for i, x in enumerate(test_labels) if not x]\n",
    "\n",
    "# partition the predictions per class\n",
    "positive_predictions = test_predictions[positive_test_indices]\n",
    "negative_predictions = test_predictions[negative_test_indices]\n",
    "\n",
    "# plot histograms of age vectors\n",
    "ax = plt.subplot(111)\n",
    "ax.hist((positive_predictions, negative_predictions), bins=50, linewidth=3)\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend(['positives (mean: %1.3f)' % np.mean(positive_predictions),\n",
    "            'negatives (mean: %1.3f)' % np.mean(negative_predictions)            \n",
    "            ], bbox_to_anchor=(1.2,1.0))\n",
    "plt.xlim([-10, 10])\n",
    "plt.title('Predictions per class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good: the histograms indicate that the predictions for positives tend to be higher than those for negatives. Now lets quantify the predictive performance.\n",
    "\n",
    "The empirical evaluation of classifiers requires a **score function**, that is an objective, quantitative criterion to assess the performance. In machine learning, many types of score functions exist, and these are often designed to meet a certain application's unique requirements (e.g., its tradeoff between the cost of false positives vs. false negatives). It is important to use an independent test set to evaluate generalization performance of a model (that is, its predictive performance on unseen data) in order to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "Most score functions for binary classification are based on entries of the so-called **contingency table**, which relates a model's predictions to the ground truth (given labels) via counts of *true positives* ($TP$: correct positive predictions), *false positives* ($FP$: incorrect positive predictions), *true negatives* ($TN$: correct negative predictions) and *false negatives* ($FN$: incorrect negative predictions).\n",
    "\n",
    "The simplest way to evaluate a classifier's predictive performance is by computing its accuracy on an independent test set. Accuracy is simply the fraction of the model's (binary) predictions (positive/negative) that were correct. However, most models provide more information than a simple binary prediction: typically a level of confidence in the prediction is also available. For example, a logistic regression classifier outputs the probability that a certain instance belongs to the positive class (ranging between 0 and 1). An SVM provides the signed distance to the separating hyperplane (ranging between $-\\infty$ and $+\\infty$), where higher absolute values imply higher confidence in the prediction. \n",
    "\n",
    "Naive score metrics that only account for a single *operating point* (like accuracy, sensitivity, specificity, ...) require choosing a certain **threshold** to turn the classifier's continuous predictions (e.g., probabilities) into binary decisions. For example, the default threshold for probabilistic models is 0.5 (50% probability that the instance is positive) while the default threshold for SVM models is 0 (instance is exactly on the separating hyperplane). Clearly, score metrics based on a single threshold fail to penalize/reward a lot of the (learned) information embedded in a model, and are hence suboptimal to compare models for many applications.\n",
    "\n",
    "In this session we will use **[receiver operating characteristic (ROC) curves](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)**, which visualize a model's true positive rate ($TPR = TP / (TP+FN)$, ranging from 0 to 1) as a function of its false positive rate ($FPR = FP / (FP + TN)$, ranging from 0 to 1) over its full operating range (that is, all possible thresholds). An ROC curve is a monotonically increasing curve. A perfect classifier's ROC curve contains the point (0, 1), which means that it identifies all positives ($TPR = 1$) without any false positives ($FPR = 0$). A commonly used summary statistic is the area under the curve (AUC), which realistically ranges between 0.5 (random model) and 1.0 (perfect model).\n",
    "\n",
    "An implementation of ROC analysis is available in [sklearn.metrics](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# the roc_curve function returns lists of false positive rates, true positive rates \n",
    "# and the corresponding thresholds as discussed previously\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, test_predictions, pos_label=True)\n",
    "AUC = roc_auc_score(test_labels, test_predictions)\n",
    "\n",
    "# we will mark two thresholds on the plot\n",
    "index_a = int(0.2 * len(thresholds))\n",
    "index_b = int(0.8 * len(thresholds))\n",
    "\n",
    "# plot the ROC curve\n",
    "plt.plot(fpr, tpr, linewidth=3)\n",
    "plt.plot([fpr[index_a], fpr[index_b]], [tpr[index_a], tpr[index_b]], 'kx', markersize=10, mew=3)\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.title('ROC curve for linear SVM classifier')\n",
    "plt.axes().axis([0, 1, 0, 1])\n",
    "d = 0.05\n",
    "plt.annotate('threshold: %1.3f' % thresholds[index_a], xy=(fpr[index_a]+d, tpr[index_a]-d))\n",
    "plt.annotate('threshold: %1.3f' % thresholds[index_b], xy=(fpr[index_b]+d, tpr[index_b]-d))\n",
    "print('The linear SVM has roughly %d%% area under the ROC curve.' % int(100 * AUC))\n",
    "\n",
    "# convenience function to summarize contingency table\n",
    "def print_performance(labels, predictions, threshold):\n",
    "    total = len(labels)\n",
    "    print('\\nPredictive performance with threshold %1.3f' % threshold)\n",
    "    binary_predictions = predictions > threshold\n",
    "    tp = sum(binary_predictions & labels)\n",
    "    fp = sum(binary_predictions & np.invert(labels))\n",
    "    tn = sum(np.invert(binary_predictions) & np.invert(labels))\n",
    "    fn = sum(np.invert(binary_predictions) & labels)\n",
    "    print('- %d true positives \\t (%d%% of total test set)' % (tp, int(100.0 * tp / total)))\n",
    "    print('- %d false positives \\t (%d%% of total test set)' % (fp, int(100.0 * fp / total)))\n",
    "    print('- %d true negatives \\t (%d%% of total test set)' % (tn, int(100.0 * tn / total)))\n",
    "    print('- %d false negatives \\t (%d%% of total test set)' % (fp, int(100.0 * fn / total)))\n",
    "    print('- accuracy: %d%%' % int(100.0 * (tp + tn) / total))\n",
    "    print('- sensitivity: %d%%' % int(100.0 * (tp) / (tp + fn)))\n",
    "    print('- specificity: %d%%' % int(100.0 * (tn) / (fp + tn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets have a look at the predictive performance at the first threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_performance(test_labels, test_predictions, thresholds[index_a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try the other threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_performance(test_labels, test_predictions, thresholds[index_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that sensitivity increases when the threshold for positive predictions decreases (i.e., more patients are labeled positive in the binary predictions), while specificity decreases. A high threshold corresponds to conservative predictions, where only the patients with highest confidence are predicted positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While building a predictive model we need to evaluate its performance before deploying it. The simplest way is to simply split the training data in to two parts: one for training and one for validation. ![Train test split](pics/fig_split-1.png)\n",
    "While this method is simple it may lead to unstable results for different splits. In order to reduce variance we can repeat multiple times the same procedure which we call $k$-fold cross-validation.\n",
    "![5 fold Cross validation](pics/fig_cv-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Let's build a cross validator.\n",
    "# We use a fixed seed in order to compare classifiers across the same folds.\n",
    "# X is the training set, y the training labels and clf the model to evaluate\n",
    "def crossValidateClassifier(X,y,clf):\n",
    "    # first we set a 5-fold cv object\n",
    "    cross_val = cross_validation.KFold(len(y),n_folds=10,shuffle=True,random_state=123)\n",
    "    # For scoring we have to define a performance measure to evaluate\n",
    "    cv_score = cross_validation.cross_val_score(clf,X,y,scoring='roc_auc',cv=cross_val)\n",
    "    #collect the scores and calculate mean and std\n",
    "    return np.mean(cv_score),np.std(cv_score)\n",
    "\n",
    "# Set here your favorite model\n",
    "model = RandomForestClassifier(n_estimators=50,max_depth=15,n_jobs=3)\n",
    "cv_mean,cv_std = crossValidateClassifier(data,labels,model)\n",
    "print (\"5-CV mean: {0:f} std: {1:f}\".format(cv_mean,cv_std))\n",
    "\n",
    "# Try to change the number of folds and observe the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code we shuffle the data. Could you imagine why? In which cases this is not advisable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with some feature selection methods in order to remove attributes that may not be necessary for the predictive models. Let's have a look again at the data rapidly. This time we use a pandas DataFrame and then printing the first rows as well as a description for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the data frame\n",
    "df = pd.DataFrame(data,columns=features)\n",
    "# Print the first rows\n",
    "print (df.head())\n",
    "# Print a description of the variables\n",
    "print (df.describe())\n",
    "\n",
    "#df['gender'].astype(int).unique()\n",
    "\n",
    "# A simple technique for feature selection is to remove the attributes with low or zero variance.\n",
    "# For this purpose we will Scikit-learn\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# here we can specify a threshold, default is zero variance\n",
    "\n",
    "var_selector = VarianceThreshold(0.0)\n",
    "new_data = var_selector.fit_transform(data)\n",
    "\n",
    "print (\"Reduced dataset contains {0:d} features\".format(new_data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the dataset contains a lot of features with zero variance. Removing these features can speed up the learning process.\n",
    "\n",
    "Now, let's try a simple method which orders the features according to their importances calculated for a certain scoring function. For example, that could be a statistical test (e.g. $\\chi^2$) or even another machine learning model (e.g. a linear model with $L_1$ regularization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest,chi2,SelectFromModel\n",
    "\n",
    "\n",
    "\n",
    "# We use a non-linear model namely Random Forest which \n",
    "# builds multiple decision trees\n",
    "model = RandomForestClassifier(n_estimators=50,max_depth=15,n_jobs=3)\n",
    "\n",
    "# We will try several values for the selected features\n",
    "k_values = np.arange(100,1000,100)\n",
    "mean = []\n",
    "stds = []\n",
    "for k in k_values:\n",
    "    X_new = SelectKBest(chi2, k=k).fit_transform(data, labels)\n",
    "    m,std = crossValidateClassifier(X_new,labels,model)\n",
    "    mean.append(m)\n",
    "    stds.append(std)\n",
    "\n",
    "m,std = crossValidateClassifier(data,labels,model)\n",
    "\n",
    "plt.errorbar(k_values,mean,yerr=stds,color=\"blue\", fmt='-o',label=\"x2\")\n",
    "plt.axhline(y=m,color='red',label=\"All features\")\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Roc-auc score')\n",
    "plt.title('#features vs. Roc-auc score')\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(50,950)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use now another predictive model in order to select a number of features and then feed them to the learner. Scikit-learn provides a wrapper SelectFromModel for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# A parameter to be tuned for the feature selector\n",
    "c_values = [0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "mean = []\n",
    "stds = []\n",
    "shapes = []\n",
    "for c in c_values:\n",
    "    # We use a linear model with L1-regularization as feature selector\n",
    "    selector = SelectFromModel(LinearSVC(C=c, penalty=\"l1\", dual=False),prefit=False)\n",
    "    X_new = selector.fit_transform(data,labels)\n",
    "    print (\"Selected {0:d} features, C = {1:f}\".format(X_new.shape[1],c))\n",
    "    mn,std = crossValidateClassifier(X_new,labels,model)\n",
    "    shapes.append(X_new.shape[1])\n",
    "    mean.append(mn)\n",
    "    stds.append(std)\n",
    "\n",
    "plt.errorbar(shapes,mean,yerr=stds,color=\"blue\", fmt='-o',label=\"L1 feature selection\")\n",
    "plt.axhline(y=m,color='red',label=\"All features\")\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Roc-auc score')\n",
    "plt.title('#features vs. Roc-auc score')\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(340,880)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Constructing a machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, we have seen sofar how to pre-process a dataset, apply features selection methods, tune parameters and train predictive models. Usually, all these steps are packed in a single pipeline in order to replicate easilly the experiments with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first import the method from Scikit-learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# We define a sequence of steps\n",
    "# Let's start with a fetures selection method followed by a random-forest learner\n",
    "\n",
    "transformers = [('select_k_best',SelectKBest(chi2)),(\"rf_model\",RandomForestClassifier())]\n",
    "pipeline = Pipeline(transformers)\n",
    "\n",
    "# set some parameters for grid searching\n",
    "# to do that we use the following convention: nameoftransformer__nameofparameter=[list of values]\n",
    "\n",
    "params = dict(select_k_best__k=np.arange(100,400,100), rf_model__n_estimators=[50,100,300], \n",
    "              rf_model__max_depth=[10, 20,30])\n",
    "\n",
    "# Do a grid search with 5-cv. Note that this results to 27*5 fits. So, it may take some time for large datasets\n",
    "# Let's split the data to speed-up things\n",
    "\n",
    "X_train,X_test,y_train,y_test = cross_validation.train_test_split(data,labels,test_size=0.7,random_state=51)\n",
    "\n",
    "tuned_pipeline = GridSearchCV(pipeline, param_grid=params, n_jobs=3, refit=True, cv=5).fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print (tuned_pipeline.best_params_)\n",
    "\n",
    "# Test these parameters. The pipeline is a convenient way to apply all the steps in new data.\n",
    "# We simply have to call the predict method as in the case of other estimators.\n",
    "\n",
    "print  (\"Accuracy: {0:f}\".format(metrics.accuracy_score(y_test, tuned_pipeline.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Exercise: repeat the same procedure by adding a further step of feature selection which removes the low variance features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
