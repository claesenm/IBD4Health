{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will do some basic data analyses. As a basis, we will use a synthetic data set containing biographical and medicinal information of 10.000 persons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Python and data primer\n",
    "\n",
    "## 0.1. Python primer\n",
    "\n",
    "In this section we will provide a concise overview of the Python language, to enable you to get started. The '#' sign is used to start a comment in Python, that lasts until the end of the line. The 'print()' command can be used to print output. The 'type()' command shows the type of its argument. Python indexing always starts at 0. For more information about Python, please refer to https://docs.python.org/3/reference/index.html.\n",
    "\n",
    "### Primitive types\n",
    "First, lets go over some primitive types in Python: integers (whole numbers), floats (real numbers), booleans (true/false) and strings (text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# integers\n",
    "a = 1\n",
    "print(type(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# floats\n",
    "b = 1.0\n",
    "type(b)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# booleans\n",
    "c = True\n",
    "print(type(c))\n",
    "print(c)\n",
    "d = a == 2\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# string\n",
    "s = 'We are the knights who say \"Ni!\"'\n",
    "print(type(s))\n",
    "print(s)\n",
    "s2 = s + ' And we demand a shrubbery!'\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in basic data types\n",
    "\n",
    "Python has a number of built-in data types, including lists, dictionaries and sets. We will briefly go over the uses and syntax of each of these.\n",
    "\n",
    "A *list* is an ordered sequence of values, not necessarily of the same type. Lists can be initialized via square brackets []."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = [1, 'blah', 2.0, True]\n",
    "print(type(l))\n",
    "print(l)\n",
    "\n",
    "# iterate over list\n",
    "for i in l:\n",
    "    print(i)\n",
    "    \n",
    "# add element to a list\n",
    "l.append(5)\n",
    "print(l)\n",
    "\n",
    "# list slicing: print the first 3 elements\n",
    "print(l[0:3])\n",
    "\n",
    "# length of the list: use the len() function\n",
    "print(len(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *dictionary* (dict) is a container that maps (unique) keys to values, and is typically initialized using curly brackets {}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = {'a': 1, 'b': 2}\n",
    "print(type(d))\n",
    "print(d)\n",
    "\n",
    "# get item 'a' from the dictionary\n",
    "print(d['a'])\n",
    "\n",
    "# length of the dictionary\n",
    "print(len(d))\n",
    "\n",
    "# iterate over the dictionary\n",
    "# note that the ordering is not necessarily as initialized\n",
    "for k, v in d.items():\n",
    "    print('key %s mapped to value %d' % (k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *set* is a container for distinct values, and can be initialized via the *set()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1 = set([1, 2, 3, 3])\n",
    "print(type(s1))\n",
    "print(s1)\n",
    "\n",
    "s2 = set([3, 4, 5])\n",
    "print(s2)\n",
    "\n",
    "# set union\n",
    "s3 = s1.union(s2)\n",
    "print(s3)\n",
    "\n",
    "# set difference\n",
    "s4 = s1.difference(s2)\n",
    "print(s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various useful functions\n",
    "\n",
    "The 'help()' function can be used to open the documentation of a certain function or data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.0 represents a float object\n",
    "help(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map() is a built-in function\n",
    "help(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions provide an elegant way to create or transform iterables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to see what range() does, use help(range)\n",
    "a = [5 * i for i in range(5)]\n",
    "print(a)\n",
    "b = [x - 1 for x in a]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is importing libraries and/or data. Python has a wealth of libraries that provide well-documented functions we can use for data analysis. In particular, we will use 'NumPy' (numerical support, esp. linear algebra), 'scikit-learn' (machine learning), 'pandas' (statistics) and matplotlib (plotting). A library can be loaded using the 'import' command and renamed via 'import x as y'. To call function x from library y, we use y.x()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # numpy\n",
    "import sklearn # scikit-learn\n",
    "import pandas as pd # pandas\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt # matplotlib\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a synthetic data set based on health expenditure data. The data is organized as a matrix, and contains biographical information, class labels and information about drug purchases.\n",
    "\n",
    "For your convenience, we created a small library for loading the data and some convenience functions. We will load the library as 'ibd4h'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ibd4health as ibd4h\n",
    "data = ibd4h.data\n",
    "features = ibd4h.features\n",
    "labels = ibd4h.labels\n",
    "colidx = ibd4h.colidx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following variables are important:\n",
    "- **data**: the data matrix, rows correspond to patients, columns to features\n",
    "- **features**: the list of features (corresponding to columns in ibd4h.data)\n",
    "- **labels**: the labels for patients (corresponding to rows, True: diabetic, False: non-diabetic)\n",
    "- **colidx**: a dictionary to facilitate retrieving columns from their string definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Data set contains info on %d patients with %d features.\" \n",
    "      % data.shape)\n",
    "print(\"Feature list has %d entries.\" \n",
    "      % features.shape)\n",
    "print(\"Label list has %d entries, of which %d are positive.\" \n",
    "      % (labels.shape[0], sum(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features in this data set include age, gender the volume of drugs purchased by the patient. Lets have a look at the first 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, feat in enumerate(features[:3]):\n",
    "    print('feature %d: %s' % (idx, feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drug volumes are categorized via codes of the anatomical therapeutic chemical (ATC) classification system (cfr. http://www.whocc.no/atc_ddd_index/ for details). You can find a description of ATC code XXX in the data set at http://www.whocc.no/atc_ddd_index/?showdescription=yes&code=XXX. For example, information on the third feature 'A01AA01', can be found at http://www.whocc.no/atc_ddd_index/?showdescription=yes&code=A01AA01.\n",
    "\n",
    "The data matrix can be indexed via `data[rowidx, colidx]`. For example, the age of the first patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ageidx = colidx['age']\n",
    "age = data[0, ageidx]\n",
    "print(age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For slicing, replace either rowidx or colidx with \":\". For example, to get a vector of all ages, we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ages = data[:, ageidx]\n",
    "print(ages.shape)\n",
    "print('Average age: %1.3f' % np.mean(ages))\n",
    "print('Minimum age: %1.3f' % np.min(ages))\n",
    "print('Maximum age: %1.3f' % np.max(ages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genderidx = colidx['gender']\n",
    "# create a list of gender indicators via list comprehension\n",
    "genders = [x > 0 for x in data[:, genderidx]]\n",
    "\n",
    "# define gender representation\n",
    "male = False\n",
    "female = True\n",
    "\n",
    "num_males = genders.count(male)\n",
    "num_females = genders.count(female)\n",
    "print('Data set contains %d males and %d females.' % (num_males, num_females))\n",
    "\n",
    "# gender distribution in diabetics\n",
    "positive_rowidx = [idx for idx, label in enumerate(labels) if label]\n",
    "print(len(positive_rowidx)) # sanity check: length of this list should be 5000\n",
    "positive_genders = [x > 0 for x in data[positive_rowidx, genderidx]]\n",
    "\n",
    "num_positive_males = positive_genders.count(male)\n",
    "num_positive_females = positive_genders.count(female)\n",
    "print('Data set contains %d positive males and %d positive females.' % (num_positive_males, num_positive_females))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(data[:, ageidx], bins=50)\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Age histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a mask to identify males and females in the data\n",
    "male = data[:, colidx['gender']] > 0\n",
    "female = np.invert(male)\n",
    "print(female[:5])\n",
    "print(male[:5])\n",
    "\n",
    "# create vectors of ages\n",
    "male_ages = data[male, ageidx]\n",
    "female_ages = data[female, ageidx]\n",
    "\n",
    "# plot histograms of age vectors\n",
    "plt.hist((male_ages, female_ages), bins=50, histtype='step')\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend(['female (mean: %1.3f)' % np.mean(female_ages),\n",
    "            'male (mean: %1.3f)' % np.mean(male_ages)],\n",
    "          loc='lower center')\n",
    "plt.title('Age histogram per gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simple predictive modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Train test split](pics/fig_split-1.png)\n",
    "![5 fold Cross validation](pics/fig_cv-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with some feature selection methods in order to remove attributes that may not be necessary for the predictive models. Let's have a look again at the data rapidly. This time we use a pandas DataFrame and then printing the first rows as well as a description for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the data frame\n",
    "df = pd.DataFrame(data,columns=features)\n",
    "# Print the first rows\n",
    "print (df.head())\n",
    "# Print a description of the variables\n",
    "print (df.describe())\n",
    "\n",
    "#df['gender'].astype(int).unique()\n",
    "\n",
    "# A simple technique for feature selection is to remove the attributes with low or zero variance.\n",
    "# For this purpose we will Scikit-learn\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# here we can specify a threshold, default is zero variance\n",
    "\n",
    "var_selector = VarianceThreshold(0.0)\n",
    "new_data = var_selector.fit_transform(data)\n",
    "\n",
    "print (\"Reduced dataset contains {0:d} features\".format(new_data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the dataset contains a lot of features with zero variance. Removing these features can speed up the learning process.\n",
    "\n",
    "Now, let's try a simple method which orders the features according to their importances calculated for a certain scoring function. For example, that could be a statistical test (e.g. $\\chi^2$) or even another machine learning model (e.g. a linear model with $L_1$ regularization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest,chi2,SelectFromModel\n",
    "\n",
    "# helper function for cross-validating a classifier\n",
    "# we use a fixed seed in order to compare classifiers\n",
    "# across the same folds\n",
    "def crossValidateClassifier(X,y,clf):\n",
    "    cross_val = cross_validation.KFold(len(y),n_folds=5,shuffle=True,random_state=224441)\n",
    "    cv_score = cross_validation.cross_val_score(clf,X,y,scoring='roc_auc',cv=cross_val)\n",
    "    return np.mean(cv_score),np.std(cv_score)\n",
    "\n",
    "# We use a non-linear model namely Random Forest which \n",
    "# builds multiple decision trees\n",
    "# ==== Marc, I suposse here we will use parameters we optimize in previously ====\n",
    "model = RandomForestClassifier(n_estimators=100,max_depth=35,n_jobs=3)\n",
    "\n",
    "# We will try several values for the selected features\n",
    "k_values = np.arange(100,1000,100)\n",
    "mean = []\n",
    "stds = []\n",
    "for k in k_values:\n",
    "    X_new = SelectKBest(chi2, k=k).fit_transform(data, labels)\n",
    "    m,std = crossValidateClassifier(X_new,labels,model)\n",
    "    mean.append(m)\n",
    "    stds.append(std)\n",
    "\n",
    "m,std = crossValidateClassifier(data,labels,model)\n",
    "\n",
    "plt.errorbar(k_values,mean,yerr=stds,color=\"blue\", fmt='-o',label=\"x2\")\n",
    "plt.axhline(y=m,color='red',label=\"All features\")\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Roc-auc score')\n",
    "plt.title('#features vs. Roc-auc score')\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(50,950)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use now another predictive model in order to select a number of features and then feed them to the learner. Scikit-learn provides a wrapper SelectFromModel for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# A parameter to be tuned for the feature selector\n",
    "c_values = [0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "mean = []\n",
    "stds = []\n",
    "shapes = []\n",
    "for c in c_values:\n",
    "    # We use a linear model with L1-regularization as feature selector\n",
    "    selector = SelectFromModel(LinearSVC(C=c, penalty=\"l1\", dual=False),prefit=False)\n",
    "    X_new = selector.fit_transform(data,labels)\n",
    "    print (\"Selected {0:d} features, C = {1:f}\".format(X_new.shape[1],c))\n",
    "    mn,std = crossValidateClassifier(X_new,labels,model)\n",
    "    shapes.append(X_new.shape[1])\n",
    "    mean.append(mn)\n",
    "    stds.append(std)\n",
    "\n",
    "plt.errorbar(shapes,mean,yerr=stds,color=\"blue\", fmt='-o',label=\"L1 feature selection\")\n",
    "plt.axhline(y=m,color='red',label=\"All features\")\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Roc-auc score')\n",
    "plt.title('#features vs. Roc-auc score')\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(340,880)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Constructing a machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, we have seen sofar how to pre-process a dataset, apply features selection methods, tune parameters and train predictive models. Usually, all these steps are packed in a single pipeline in order to replicate easilly the experiments with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first import the method from Scikit-learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# We define a sequence of steps\n",
    "# Let's start with a fetures selection method followed by a random-forest learner\n",
    "\n",
    "transformers = [('select_k_best',SelectKBest(chi2)),(\"rf_model\",RandomForestClassifier())]\n",
    "pipeline = Pipeline(transformers)\n",
    "\n",
    "# set some parameters for grid searching\n",
    "# to do that we use the following convention: nameoftransformer__nameofparameter=[list of values]\n",
    "\n",
    "params = dict(select_k_best__k=np.arange(100,400,100), rf_model__n_estimators=[50,100,300], \n",
    "              rf_model__max_depth=[10, 20,30])\n",
    "\n",
    "# Do a grid search with 5-cv. Note that this results to 27*5 fits. So, it may take some time for large datasets\n",
    "# Let's split the data to speed-up things\n",
    "\n",
    "X_train,X_test,y_train,y_test = cross_validation.train_test_split(data,labels,test_size=0.7,random_state=51)\n",
    "\n",
    "tuned_pipeline = GridSearchCV(pipeline, param_grid=params, n_jobs=3, refit=True, cv=5).fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print (tuned_pipeline.best_params_)\n",
    "\n",
    "# Test these parameters. The pipeline is a convenient way to apply all the steps in new data.\n",
    "# We simply have to call the predict method as in the case of other estimators.\n",
    "\n",
    "print  (\"Accuracy: {0:f}\".format(metrics.accuracy_score(y_test, tuned_pipeline.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Exercise: repeat the same procedure by adding a further step of feature selection which removes the low variance features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
